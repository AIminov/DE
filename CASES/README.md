# Реальные задачи на работе


### КЕЙС - Витрина данных

**Витрина данных** – это просто табличка, которая уже сто раз отфильтрована, где-то очищена от лишних данных. Но у витрины данных могут быть разные источники. Например у меня витрина собиралась из данных по дебетовым картам, кредитным картам, ипотекам, вкладам и так далее. Реально много разных источников-таблиц. Каждый источник обновляет у себя данные в разное время. У кого-то данные готовы за вчера, а где-то только за позавчера. 

Поэтому если мы будет запускать расчет витрины с одной датой за вчера (Т-1), то по каким-то источникам мы прочитаем свежие данные, а по каким-то будет 0 строк (данные в источнике еще не добавились). Че делать?

Я начал логировать в отдельную таблицу последнюю дату загрузки данных по каждому продукту во время расчета витрины. И при следующем запуске, мой скрипт ходит в таблицу с метаданными и читает для каждого продукта (карты, кредиты, вклады и т.д.) свою максимальную дату.

| Название продукта     | Данные загружены |
|-----------------------|---------------|
| Дебетовые карты       | 2024-11-10    |
| Кредитные карты       | 2024-11-11    |
| Ипотека               | 2024-11-15    |
| Вклады                | 2024-11-11    |
| Переводы СНГ         | 2024-11-18    |


Очевидно, что искать максимальную дату в такой таблице это очень быстро. Ну вот мы видим, что по дебетовым картам крайняя дата загрузки 2024-11-10. Следовательно, в следующий раз данные начнут грузиться за 11 ноября. А по Ипотеке уже с 16 ноября. При этом дата, до которой скрипту надо грузиться тоже не с неба берется. Я также читаю дату загрузки у самого источника. И делаю это не в лоб. Ну источник может быть держать данные за 10 лет и там сотни Террабайт данных. Делаю это через чтение метаданных о партициях:

```python
source_table = spark.sql(f'SHOW PARTITIONS {TABLE}')\
                            .select(F.regexp_extract("partition", f"date_event=(.*)", 1).alias("date"))\
                            .select(F.max("date").alias("max_date"))
                            .first()["max_date"]
```

Можем представить, что источник по Вкладам сдох и таблица с вкладами новых данных не имеет. Через месяц ее починят и наш скрипт начнет с той даты, которая была последней в витрине данных. При этом другие продукты никак не пострадают. 

***
### КЕЙС - Использование dbt

Ко мне пришли аналитики. У них было три SQL скрипта длиной в 500-800 строк. В этих SQL было очень много операций CASE WHEN и дальше какое-то условие, типа
```sql
SELECT
    CASE 
        WHEN URL LIKE '%business%' THEN 'camp_business'
        WHEN URL LIKE '%business1%' THEN 'camp_business1'
        WHEN URL LIKE '%business2%' THEN 'camp_business2'
        ... (500 условий)
        ELSE 'NO'
    END
FROM TABLE
```
У них это все невозможно было читать, рефакторить, да и они там потом еще делали несколько UNION и он падал, когда это можно было и не делать. Ну просто хардкодом были написаны скрипты. 
Я взял dbt и написал модельку типа:
```sql
{{ config(
    materialized='incremental',
    engine='ReplicatedMergeTree',
    tags=['etea']
) }}


# Read dbt_marks
{% set sql %}
    select * from {{ ref('dbt_marks') }}
{% endset %}
{% set res = run_query(sql) %}



{% set exec_date = var('execution_date')|string %}
{{ log("exec_date: " ~ exec_date, True) }}



# Save to condition_list, result_list
{% if execute %}

    {% set condition_list = res.columns[0].values() %}
    {% set result_list = res.columns[1].values() %}

{% else %}

    {% set results_list = [] %}

{% endif %}



{% for condition, result in zip(condition_list, result_list) %}
    SELECT
        dateTime,
        CASE
            WHEN {{ condition }} THEN {{ result }}
            ELSE 'undefined'
        END as URL,
        counterUserIDHash,
        isPageView
    FROM {{ source('schema', 'your_table') }}
    WHERE dateTime::DATE = '{{ exec_date }}'
    AND isPageView = '1'
    AND URL not like '%stage%'
    {% if not loop.last %}
        union all
    {% endif %}
{% endfor %}
```
Тут можно увидеть много непонятного, но присмотритесь к главному SQL скрипту. Там видно, что в условия CASE WHEN подставляется некий шаблон. У меня есть цикл, внутри которого запускается SQL запрос с ОДНИМ CASE WHEN. В этом цикле в условия поочередно подставляются значения. Эти значения берутся из обычного txt файлика. 

Вот так выглядит файлик:
```
condition,result
URL like '%business/ecom/smm%','smm'
URL like '%business/ecom/all%','all'
URL like '%business/nbs/insales%','insales'
...
```

И теперь аналитики могут просто добавлять сколько угодно своих дополнительных условий в этот файлик, а dbt будет автоматически читать оттуда все строчки и подставлять в запрос и выполнять его!

И теперь вместо 800 строк SQL кода у нас есть простой SQL запрос и txt файлик.

***
### КЕЙС - Из старого Greenplum в новый Greenplum

Была ситуация - был старый кластер Greenplum. В нем были таблицы, которые надо было перенести в новый Greenplum. Перетаскивать можно разными способами.

Вот первый способ в лоб:
Берем Apache Spark, подключаемся по jdbc к Гринпламу, пишем запрос и читаем данные. НО! В случае чтения Спарком через JDBC у нас все даные льются через мастерхост. То самое бутылочное горлышко. Здесь можно обмануть всех и читать по отдельности с каждого сегмента Greenplum. Например добавить фильтр WHERE gp_segment_id = 0(1, 2, 3, 4 и так далее). Тогда мы сильно разгрузим матсерхост.

Второй вариант через гринплам коннектор:
Вот тут уже можно читать и записывать данные параллельно с разных сегментов одновременно. Нет того самого бутылочного горлышка на стороне ГП. Но при таком коннекторе у меня читалась вся таблица целиком типа ```SELECT * FROM TABLE```. Не вариант. Я не смог найти ответ, как задавать именно конкретный запрос, используя этот коннектор. Идем дальше.

Третий вариант через EXTERNAL TABLE:
В самом Dbeaver в Greenplum создаем внешнюю таблицу в S3 с помощью PXF. Короче в гринплам создается типа ссылка на данные в S3 (прям указывается конкретный путь до папки). Запрос пишем в ГП, а данные лежат в S3. Но при этом чтение все, как будто пользуемся обычной таблицей. Супер! Значит в старом Гринпламе создаем внешнюю таблицу в S3. И в новом Гринпламе создаем внешнюю таблицу в S3 на ту же папку! Т.е. у нас два ГП читают из одного места.

**Пример кода на создание внешней таблицы в Greenplum**.
При этом данные сгружаются в S3 и их можно прям потрогать

**Запись из Greenplum в S3**
```sql
Drop EXTERNAL TABLE if exists pxf_write_paqruet_s3;
CREATE WRITABLE EXTERNAL TABLE pxf_write_paqruet_s3 (
 visit_id text,
 body text,
 meta__checkpoint text
)
LOCATION ('pxf://data-lake/test/?PROFILE=s3:parquet&accesskey=***&secretkey=***&endpoint=https://storage.yandexcloud.net')
FORMAT 'CUSTOM' (FORMATTER='pxfwritable_export');
INSERT into pxf_write_paqruet_s3
select *
from stg2.table
where meta__checkpoint = '2024-10-01';
```

**Чтение из S3 в Greenplum**
```sql
Drop EXTERNAL TABLE if exists pxf_read_paqruet_s3;
CREATE READABLE EXTERNAL TABLE pxf_read_paqruet_s3 (
 visit_id text,
 body text,
 meta__checkpoint text
)
LOCATION ('pxf://data-lake/test/?PROFILE=s3:parquet&accesskey=***&secretkey=***&endpoint=https://storage.yandexcloud.net')
FORMAT 'CUSTOM' (FORMATTER='pxfwritable_import');

select *
from pxf_read_paqruet_s3;
```

Собственно в новом ГП мы делаем тоже самое, но уже не ``` INSERT INTO ```, а просто читаем, типа ``` SELECT * FROM TABLE ```. Это займет конечно время, если таблица огромная, но это всяко легче и быстрее, чем два предыдущих способа.