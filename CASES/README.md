# Реальные задачи на работе


### КЕЙС Витрина данных

**Витрина данных** – это просто табличка, которая уже сто раз отфильтрована, где-то очищена от лишних данных. Но у витрины данных могут быть разные источники. Например у меня витрина собиралась из данных по дебетовым картам, кредитным картам, ипотекам, вкладам и так далее. Реально много разных источников-таблиц. Каждый источник обновляет у себя данные в разное время. У кого-то данные готовы за вчера, а где-то только за позавчера. 

Поэтому если мы будет запускать расчет витрины с одной датой за вчера (Т-1), то по каким-то источникам мы прочитаем свежие данные, а по каким-то будет 0 строк (данные в источнике еще не добавились). Че делать?

Я начал логировать в отдельную таблицу последнюю дату загрузки данных по каждому продукту во время расчета витрины. И при следующем запуске, мой скрипт ходит в таблицу с метаданными и читает для каждого продукта (карты, кредиты, вклады и т.д.) свою максимальную дату.

| Название продукта     | Данные загружены |
|-----------------------|---------------|
| Дебетовые карты       | 2024-11-10    |
| Кредитные карты       | 2024-11-11    |
| Ипотека               | 2024-11-15    |
| Вклады                | 2024-11-11    |
| Переводы СНГ         | 2024-11-18    |


Очевидно, что искать максимальную дату в такой таблице это очень быстро. Ну вот мы видим, что по дебетовым картам крайняя дата загрузки 2024-11-10. Следовательно, в следующий раз данные начнут грузиться за 11 ноября. А по Ипотеке уже с 16 ноября. При этом дата, до которой скрипту надо грузиться тоже не с неба берется. Я также читаю дату загрузки у самого источника. И делаю это не в лоб. Ну источник может быть держать данные за 10 лет и там сотни Террабайт данных. Делаю это через чтение метаданных о партициях:

```python
source_table = spark.sql(f'SHOW PARTITIONS {TABLE}')\
                            .select(F.regexp_extract("partition", f"date_event=(.*)", 1).alias("date"))\
                            .select(F.max("date").alias("max_date"))
                            .first()["max_date"]
```

Можем представить, что источник по Вкладам сдох и таблица с вкладами новых данных не имеет. Через месяц ее починят и наш скрипт начнет с той даты, которая была последней в витрине данных. При этом другие продукты никак не пострадают. 

***
### КЕЙС Использование dbt

Ко мне пришли аналитики. У них было три SQL скрипта длиной в 500-800 строк. В этих SQL было очень много операций CASE WHEN и дальше какое-то условие, типа
```sql
SELECT
    CASE 
        WHEN URL LIKE '%business%' THEN 'camp_business'
        WHEN URL LIKE '%business1%' THEN 'camp_business1'
        WHEN URL LIKE '%business2%' THEN 'camp_business2'
        ... (500 условий)
        ELSE 'NO'
    END
FROM TABLE
```
У них это все невозможно было читать, рефакторить, да и они там потом еще делали несколько UNION и он падал, когда это можно было и не делать. Ну просто хардкодом были написаны скрипты. 
Я взял dbt и написал модельку типа:
```sql
{{ config(
    materialized='incremental',
    engine='ReplicatedMergeTree',
    tags=['etea']
) }}


# Read dbt_marks
{% set sql %}
    select * from {{ ref('dbt_marks') }}
{% endset %}
{% set res = run_query(sql) %}



{% set exec_date = var('execution_date')|string %}
{{ log("exec_date: " ~ exec_date, True) }}



# Save to condition_list, result_list
{% if execute %}

    {% set condition_list = res.columns[0].values() %}
    {% set result_list = res.columns[1].values() %}

{% else %}

    {% set results_list = [] %}

{% endif %}



{% for condition, result in zip(condition_list, result_list) %}
    SELECT
        dateTime,
        CASE
            WHEN {{ condition }} THEN {{ result }}
            ELSE 'undefined'
        END as URL,
        counterUserIDHash,
        isPageView
    FROM {{ source('schema', 'your_table') }}
    WHERE dateTime::DATE = '{{ exec_date }}'
    AND isPageView = '1'
    AND URL not like '%stage%'
    {% if not loop.last %}
        union all
    {% endif %}
{% endfor %}
```
Тут можно увидеть много непонятного, но присмотритесь к главному SQL скрипту. Там видно, что в условия CASE WHEN подставляется некий шаблон. У меня есть цикл, внутри которого запускается SQL запрос с ОДНИМ CASE WHEN. В этом цикле в условия поочередно подставляются значения. Эти значения берутся из обычного txt файлика. 

Вот так выглядит файлик:
```
condition,result
URL like '%business/ecom/smm%','smm'
URL like '%business/ecom/all%','all'
URL like '%business/nbs/insales%','insales'
...
```

И теперь аналитики могут просто добавлять сколько угодно своих дополнительных условий в этот файлик, а dbt будет автоматически читать оттуда все строчки и подставлять в запрос и выполнять его!

И теперь вместо 800 строк SQL кода у нас есть простой SQL запрос и txt файлик.

<!-- Сборка витрины на dbt для аналитиков 

Миграция из ГП в ГП 700Гб через jdbc и через pxf.

Из источника прилетает строка с //u0000, которую нельзя кастануть в json. Надо исправлять это на уровне Спарка

Миграция 700Гб из ГП в ГП таблицы через pxf external table. Также через jdbc по сегментам. 
Миграция из AWS в GP с помощью Spark Scala.
CI CD на Github Actions (проверка изменений в файлах с Github Summary)
Переписывание API на питоне на ГринПлам с использованием Метадаты

есть папка 1000 файлов
названы по разному
Мне надо загрузить это все в БД
Взять все - слишком много
Надо брать частями - вопрос как это сделать? --> -->
