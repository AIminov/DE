# Реальные задачи на работе


### КЕЙС Витрина данных

**Витрина данных** – это просто табличка, которая уже сто раз отфильтрована, где-то очищена от лишних данных. Но у витрины данных могут быть разные источники. Например у меня витрина собиралась из данных по дебетовым картам, кредитным картам, ипотекам, вкладам и так далее. Реально много разных источников-таблиц. Каждый источник обновляет у себя данные в разное время. У кого-то данные готовы за вчера, а где-то только за позавчера. 

Поэтому если мы будет запускать расчет витрины с одной датой за вчера (Т-1), то по каким-то источникам мы прочитаем свежие данные, а по каким-то будет 0 строк (данные в источнике еще не добавились). Че делать?

Я начал логировать в отдельную таблицу последнюю дату загрузки данных по каждому продукту во время расчета витрины. И при следующем запуске, мой скрипт ходит в таблицу с метаданными и читает для каждого продукта (карты, кредиты, вклады и т.д.) свою максимальную дату.

| Название продукта     | Данные загружены |
|-----------------------|---------------|
| Дебетовые карты       | 2024-11-10    |
| Кредитные карты       | 2024-11-11    |
| Ипотека               | 2024-11-15    |
| Вклады                | 2024-11-11    |
| Переводы СНГ         | 2024-11-18    |


Очевидно, что искать максимальную дату в такой таблице это очень быстро. Ну вот мы видим, что по дебетовым картам крайняя дата загрузки 2024-11-10. Следовательно, в следующий раз данные начнут грузиться за 11 ноября. А по Ипотеке уже с 16 ноября. При этом дата, до которой скрипту надо грузиться тоже не с неба берется. Я также читаю дату загрузки у самого источника. И делаю это не в лоб. Ну источник может быть держать данные за 10 лет и там сотни Террабайт данных. Делаю это через чтение метаданных о партициях:

```python
source_table = spark.sql(f'SHOW PARTITIONS {TABLE}')\
                            .select(F.regexp_extract("partition", f"date_event=(.*)", 1).alias("date"))\
                            .select(F.max("date").alias("max_date"))
                            .first()["max_date"]
```

Можем представить, что источник по Вкладам сдох и таблица с вкладами новых данных не имеет. Через месяц ее починят и наш скрипт начнет с той даты, которая была последней в витрине данных. При этом другие продукты никак не пострадают. 





<!-- Сборка витрины на dbt для аналитиков 

Миграция из ГП в ГП 700Гб через jdbc и через pxf.

Из источника прилетает строка с //u0000, которую нельзя кастануть в json. Надо исправлять это на уровне Спарка

Миграция 700Гб из ГП в ГП таблицы через pxf external table. Также через jdbc по сегментам. 
Миграция из AWS в GP с помощью Spark Scala.
CI CD на Github Actions (проверка изменений в файлах с Github Summary)
Переписывание API на питоне на ГринПлам с использованием Метадаты

есть папка 1000 файлов
названы по разному
Мне надо загрузить это все в БД
Взять все - слишком много
Надо брать частями - вопрос как это сделать? --> -->
