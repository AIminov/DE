## Куратор раздела

<img align="left" width="200" src="../png/amp.jpg" />

**Подвальный Артем** 

   [Telegram канал](https://t.me/dataengineerlab)

Хочешь перейти в дата-инженерию, но не знаешь с чего начать? Пиши -  составим резюме, продумаем твое развитие [https://t.me/ampodvalniy](https://t.me/ampodvalniy)
 

Хочешь улучшить текущий раздел, внести недостающее или поправить формулировку? Предлагай PR и тегай [@Artemlin6231](https://github.com/Artemlin6231)

## Немного об этой главе
Apache spark - это распределенный фреймворк обработки данных, ставший де-факто стандартом в обработке больших данных. Требуется практически повсевместо в работе дата-инженера.

Приятного изучения)

# Основы Apache Spark и RDD

## Введение

**Apache Spark** — это масштабируемая платформа для распределённой обработки данных, которая позволяет выполнять вычисления в памяти, обеспечивая высокую производительность и гибкость. Она подходит как для пакетной (batch), так и для потоковой (streaming) обработки данных.

---

##  Ключевые идеи Apache Spark

-  **Эффективная DAG-модель вычислений**  
  Spark строит направленный ациклический граф (DAG), отображающий зависимости между этапами обработки. Это обеспечивает более гибкое и оптимизированное планирование по сравнению с классическим MapReduce.

- **Ленивая модель исполнения**  
  Преобразования не выполняются сразу. Spark откладывает их выполнение до вызова операции-действия, что позволяет эффективно планировать и объединять задачи.

- **Гибкое управление памятью**  
  - Предпочтение хранения данных в памяти
  - Сброс данных на диск при нехватке ресурсов
  - Возможность комбинированного хранения
  - Поддержка различных форматов сериализации

- **Широкая поддержка языков**  
  Поддерживаются API для **Scala**, **Java**, **Python** и **R**

- **Единый API для batch и streaming обработки**

---

## RDD (Resilient Distributed Dataset)

**RDD** — основная абстракция данных в Spark, представляющая собой неизменяемую, распределённую коллекцию объектов.

### Основные свойства RDD:

- **Неизменяемость и отказоустойчивость (fault tolerance)**  
  Все операции над RDD являются детерминированными и безопасными к сбоям.

- **Два типа операций:**
  - **Transformations** — возвращают новый RDD, операции ленивые  
    Примеры: `map`, `filter`, `join`
  - **Actions** — инициируют выполнение вычислений  
    Примеры: `count`, `collect`, `save`

- **Партиционирование**  
  Данные разбиты на независимые части (partition), которые обрабатываются параллельно.

- **Кэширование данных**  
  Поддерживаются различные уровни хранения:
  - `memory`
  - `disk`
  - `memory & disk`
  - `external*` (при внешней настройке)
 
  
## ⚙Архитектура и модель вычислений Spark

<p align="center">
    <img src="./../png/spark_arch.png" alt="hadoop"/>
</p>

### Компоненты архитектуры

- **Driver Program**  
  Главный управляющий процесс, с которого начинается выполнение приложения Spark. Он:
  - инициализирует `SparkContext`;
  - строит DAG вычислений;
  - управляет разбиением на задачи;
  - распределяет задачи по `executors`.

- **SparkContext**  
  Ядро взаимодействия приложения с кластером. Он:
  - подключается к `Cluster Manager`;
  - планирует вычисления;
  - отслеживает выполнение задач и собирает результаты.

- **Cluster Manager**  
  Менеджер ресурсов, который:
  - отслеживает доступные ресурсы;
  - выделяет `worker`-узлы;
  - запускает `executors`.  
  Поддерживаемые варианты: **Standalone**, **YARN**, **Kubernetes**, **Mesos**.

- **Worker Node**  
  Узел, на котором исполняются задачи Spark. Каждый узел может запускать один или несколько `executors`.

- **Executor**  
  Процесс, который:
  - исполняет задачи;
  - кэширует промежуточные данные;
  - взаимодействует с `driver` для отправки результатов.

- **Task**  
  Минимальная единица вычислений в Spark. Каждое преобразование над данными разбивается на множество `tasks`, которые распределяются между `executors`.

- **Cache**  
  Используется для хранения данных в памяти или на диске с целью ускорения повторных вычислений.

---

### Как работает модель вычислений Spark

1. Пользователь пишет Spark-программу с использованием `RDD`, `DataFrame` или `Dataset`.
2. Программа запускается, и `Driver` создаёт `SparkContext`.
3. Spark строит **DAG (направленный ациклический граф)** всех ленивых операций (`transformations`).
4. DAG разбивается на **этапы (stages)**, каждый из которых состоит из задач (`tasks`).
5. Через `Cluster Manager`, Spark запускает `executor`'ы на `worker`-узлах.
6. `Tasks` исполняются параллельно на `executors`.
7. Промежуточные данные могут кэшироваться.
8. После завершения все результаты собираются и возвращаются `Driver`-у.



Ниже полезные материалы, которые помогут в освоении Spark:
- [Теория по Spark в PDF](../files/spark.pdf)
- [Онлайн расчет ресурсов для Spark](https://sparkconfigoptimizer.com)
- [Как пользоваться Spark UI?](https://habr.com/ru/companies/avito/articles/764996/)
- [Leetcode по PySpark](https://platform.stratascratch.com/coding?code_type=6)


Одна из хороших практик для обучения - переписывать запросы с SQL на Spark и наоборот.
Результаты, очевидно, должны быть одинаковыми.

**Вот пример двух одинаковых запросов на SQL и на PySpark**

```sql
SELECT
    d.department_name,
    AVG(s.salary) AS average_salary
FROM employees e
    JOIN departments d ON e.department_id = d.department_id
    JOIN salaries s ON e.employee_id = s.employee_id
WHERE s.salary >= 3000
GROUP BY d.department_name
ORDER BY average_salary DESC;
```


```python
result_df = employees_df\
                .join(departments_df, "department_id")\
                .join(salaries_df, "employee_id")
                .filter(salaries_df.salary >= 3000)
                .groupBy("department_name")
                .agg(F.avg("salary").alias("average_salary"))
                .orderBy(F.desc("average_salary"))
result_df.show()
```
